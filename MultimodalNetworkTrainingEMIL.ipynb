{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kAVXf8Owdj_"
   },
   "source": [
    "# Multimodal Classification Training\n",
    "\n",
    "This notebook creates the Multimodal Network Architecture and trains it for grasp testset1. After training the network weights will be stored in the folder `./dataset/grasp_testset1_logs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9PQVYgswdkG"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "`Python 3.5.4` is used for development and following packages are required to run the code provided in the notebook:\n",
    "\n",
    "`pip install googledrivedownloader`<br>\n",
    "`pip install matplotlib`<br>\n",
    "`pip install tensorflow-gpu`<br>\n",
    "`pip install keras`<br>\n",
    "`pip install numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JN0aShvGw1pt",
    "outputId": "be7b5555-28a1-426b-83d8-a9aa76130d78"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Onu-rrTryUqx",
    "outputId": "aefb2778-7e10-46e3-8aae-b6a9bed129fc"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15.0\n",
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tkd2IbswwdkJ"
   },
   "outputs": [],
   "source": [
    "import os, csv, time, shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "path=\"/content/drive/MyDrive/Deep-Multi-Sensory-Object-Categorization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPSOzNnZwdkT"
   },
   "outputs": [],
   "source": [
    "def print_image(image, title):\n",
    "    \"\"\"Print the image\n",
    "\n",
    "    :param image: image pixels in list\n",
    "    :param title: title as string to be printed on top of the image\n",
    "    \"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def time_taken(start, end):\n",
    "    \"\"\"Human readable time between `start` and `end`\n",
    "\n",
    "    :param start: time.time()\n",
    "    :param end: time.time()\n",
    "    :returns: day:hour:minute:second\n",
    "    \"\"\"\n",
    "    time = end-start\n",
    "    day = time // (24 * 3600)\n",
    "    time = time % (24 * 3600)\n",
    "    hour = time // 3600\n",
    "    time %= 3600\n",
    "    minutes = time // 60\n",
    "    time %= 60\n",
    "    seconds = time\n",
    "    day_hour_min_sec = str('%02d' % int(day))+\":\"+str('%02d' % int(hour))+\":\"+str('%02d' % int(minutes))+\":\"+str('%02d' % int(seconds))\n",
    "    \n",
    "    return day_hour_min_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu6j9CyBwdkb"
   },
   "source": [
    "## Video Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LNTE3fJwdkb"
   },
   "outputs": [],
   "source": [
    "file_0 = path+\"/dataset/EMILver1_preprocessed/EMILver1_vi_vgg16fc2_pca19/arr_0.npy\"\n",
    "file_1 = path+\"/dataset/EMILver1_preprocessed/EMILver1_vi_vgg16fc2_pca19/arr_1.npy\"\n",
    "file_2 = path+\"/dataset/EMILver1_preprocessed/EMILver1_vi_vgg16fc2_pca19/arr_2.npy\"\n",
    "video_frames = np.load(file_0, allow_pickle=True)\n",
    "action_label = np.load(file_1, allow_pickle=True)\n",
    "object_label = np.load(file_2, allow_pickle=True)\n",
    "for i in range(len(video_frames)):\n",
    "    a01 = video_frames[i]\n",
    "    while len(a01) < 658:\n",
    "        a01 = np.concatenate((a01, np.zeros((1, a01.shape[1]))))\n",
    "    video_frames[i] = a01\n",
    "video_frames = np.array(list(video_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGFHus1wwdkg"
   },
   "source": [
    "## Sound Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81_vuxgkwdki"
   },
   "outputs": [],
   "source": [
    "file_0 = path+\"/dataset/EMILver1_preprocessed/EMILver1_au_features/arr_0.npy\"\n",
    "file_1 = path+\"/dataset/EMILver1_preprocessed/EMILver1_au_features/arr_1.npy\"\n",
    "file_2 = path+\"/dataset/EMILver1_preprocessed/EMILver1_au_features/arr_2.npy\"\n",
    "audio_frames = np.load(file_0, allow_pickle=True)\n",
    "action_label = np.load(file_1, allow_pickle=True)\n",
    "object_label = np.load(file_2, allow_pickle=True)\n",
    "for i in range(len(audio_frames)):\n",
    "    a01 = audio_frames[i]\n",
    "    while len(a01) < 658:\n",
    "        a01 = np.concatenate((a01, np.zeros((1, a01.shape[1]))))\n",
    "    audio_frames[i] = a01\n",
    "audio_frames = np.array(list(audio_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnVpmohnwdkm"
   },
   "source": [
    "## Haptic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcL3KBdRwdkm"
   },
   "outputs": [],
   "source": [
    "file_0 = path+\"/dataset/EMILver1_preprocessed/EMILver1_sm_features/arr_0.npy\"\n",
    "file_1 = path+\"/dataset/EMILver1_preprocessed/EMILver1_sm_features/arr_1.npy\"\n",
    "file_2 = path+\"/dataset/EMILver1_preprocessed/EMILver1_sm_features/arr_2.npy\"\n",
    "haptic_frames = np.load(file_0, allow_pickle=True)\n",
    "action_label = np.load(file_1, allow_pickle=True)\n",
    "object_label = np.load(file_2, allow_pickle=True)\n",
    "for i in range(len(haptic_frames)):\n",
    "    a01 = haptic_frames[i]\n",
    "    while len(a01) < 658:\n",
    "        a01 = np.concatenate((a01, np.zeros((1, a01.shape[1]))))\n",
    "    haptic_frames[i] = a01\n",
    "haptic_frames = np.array(list(haptic_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDTN0bc0xXDI"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "num_classes = np.nanmax(action_label)+1\n",
    "action_label_one_hot = np.zeros((len(action_label), num_classes)).astype(int)\n",
    "for i in range(len(action_label)):\n",
    "    action_label_one_hot[i, action_label[i]] = 1\n",
    "\n",
    "num_classes = np.nanmax(object_label)+1\n",
    "object_label_one_hot = np.zeros((len(object_label), num_classes)).astype(int)\n",
    "for i in range(len(object_label)):\n",
    "    object_label_one_hot[i, object_label[i]] = 1\n",
    "\n",
    "# train-test-split\n",
    "num_data = len(object_label)\n",
    "train_id, test_id = train_test_split(np.array(range(num_data)), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eoPqSX5wdko"
   },
   "source": [
    "## Multimodal Network Hyper-parameters\n",
    "\n",
    "This network was trained for 300 epochs using Adam optimization with learning rate 1 x $10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4MOKNHywdko",
    "outputId": "cafe0028-950e-4918-9698-d0aa84924789"
   },
   "outputs": [],
   "source": [
    "# Network hyper-parameters\n",
    "batch = 5\n",
    "training_epochs = 300\n",
    "display_step = 1\n",
    "\n",
    "behavior = \"grasp\"\n",
    "testset = \"testset1\"\n",
    "folder_name = behavior+'_'+testset\n",
    "model_path = path+\"/dataset/\"+folder_name+\"_logs/model.ckpt\"\n",
    "logs_path = path+\"/dataset/\"+folder_name+\"_logs/\"\n",
    "\n",
    "# num_classes = category_label_train_one_hot.shape[1]\n",
    "num_classes = object_label_one_hot.shape[1]\n",
    "\n",
    "Y = tf.placeholder('float', [None, num_classes], name='LabelData')\n",
    "print(\"Y: \", Y)\n",
    "\n",
    "video_frames_max = 658\n",
    "video_size = video_frames.shape[2]\n",
    "video_X = tf.placeholder('float', [None, video_frames_max, video_size], name='InputData')\n",
    "\n",
    "audio_frames_max = 658\n",
    "audio_size = audio_frames.shape[2]\n",
    "audio_keep_prob = tf.placeholder_with_default(1.0, shape=(), name='audio_keep')\n",
    "\n",
    "haptic_frames_max = 658\n",
    "haptic_size = haptic_frames.shape[2]\n",
    "haptic_keep_prob = tf.placeholder_with_default(1.0, shape=(), name='haptic_keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5nIyYWewdkq"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions used to define models\n",
    "\"\"\"\n",
    "\n",
    "haptic_skip_2nd_maxpool = [\"grasp\", \"hold\", \"low\"]\n",
    "\n",
    "def model(video_data_placeholder):\n",
    "    with tf.name_scope(\"Model\"):\n",
    "        # Video\n",
    "        net = tf.layers.flatten(video_data_placeholder)\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net)\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net)\n",
    "        video_logits = tf.layers.dense(inputs=net, units=num_classes, activation=tf.nn.relu)\n",
    "        \n",
    "        # Audio\n",
    "        audio_data_placeholder = tf.placeholder('float', [None, audio_frames_max, audio_size], name='audio_InputData')\n",
    "        net = tf.layers.flatten(audio_data_placeholder)\n",
    "        # Dense Layer\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net, rate=audio_keep_prob)\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net, rate=audio_keep_prob)\n",
    "        audio_logits = tf.layers.dense(inputs=net, units=num_classes, activation=tf.nn.relu)\n",
    "        \n",
    "        # Haptic\n",
    "        haptic_data_placeholder = tf.placeholder('float', [None, haptic_frames_max, haptic_size], name='haptic_InputData')\n",
    "        net = tf.layers.flatten(haptic_data_placeholder)\n",
    "        # Dense Layer\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net, rate=haptic_keep_prob)\n",
    "        haptic_logits = tf.layers.dense(inputs=net, units=num_classes, activation=tf.nn.relu)\n",
    "        \n",
    "        # Concatenate \n",
    "        logits = tf.concat([video_logits, audio_logits, haptic_logits], axis=1)\n",
    "        logits = tf.nn.relu(logits)\n",
    "        logits = tf.layers.dense(inputs=logits, units=num_classes)\n",
    "        \n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(prediction, label_placeholder):\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=label_placeholder))\n",
    "        # Create a summary to monitor cost tensor\n",
    "        cost_scalar = tf.summary.scalar(\"loss\", cost)\n",
    "    return cost, cost_scalar\n",
    "\n",
    "def training(prediction, label_placeholder):\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = optimizer.minimize(cost)\n",
    "    return train_op\n",
    "\n",
    "def evaluate(prediction, Y):\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        accuracy_scalar = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    return accuracy, accuracy_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9YnsTz-wdkr",
    "outputId": "6c05ca52-9988-4619-dc09-465e69ca24bc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating the Neural Network\n",
    "\"\"\"\n",
    "\n",
    "model_dict = {}\n",
    "prediction = model(video_X)\n",
    "model_dict[\"Model\"] = prediction\n",
    "\n",
    "cost, cost_scalar = loss(prediction, Y)\n",
    "model_dict[\"Loss\"] = cost\n",
    "model_dict[\"Loss_scalar\"] = cost_scalar\n",
    "\n",
    "train_op = training(prediction, Y)\n",
    "model_dict[\"Optimizer\"] = train_op\n",
    "\n",
    "eval_op, accuracy_scalar = evaluate(prediction, Y)\n",
    "model_dict[\"Accuracy\"] = eval_op\n",
    "model_dict[\"Accuracy_scalar\"] = accuracy_scalar\n",
    "\n",
    "print(\"model_dict: \", model_dict)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZj8uywlwdks"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(logs_path):\n",
    "    shutil.rmtree(logs_path)\n",
    "    os.makedirs(logs_path)\n",
    "else:\n",
    "    os.makedirs(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EymUeUC2wdks"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Writing 'Time', 'Epoch', 'Cost', 'Accuracy' in CSV file\n",
    "\"\"\"\n",
    "\n",
    "epoch_cost_accuracy = []\n",
    "epoch_cost_accuracy.append(\"Time\")\n",
    "epoch_cost_accuracy.append(\"Epoch\")\n",
    "epoch_cost_accuracy.append(\"Cost\")\n",
    "epoch_cost_accuracy.append(\"Accuracy\")\n",
    "\n",
    "with open(logs_path+folder_name+\"_data.csv\",'w') as f:\n",
    "    writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "    writer.writerow(epoch_cost_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRcpBIjXwdks"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-ghKlW9wdkt",
    "outputId": "3f7d719b-9394-44e7-a2c6-4d42978e0d91",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"## Training\"\"\"\n",
    "\n",
    "# Start Training\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost_list = 0.0\n",
    "        total_batch = int(len(train_id)/batch)\n",
    "        \n",
    "        # Shuffle data\n",
    "        np.random.shuffle(train_id)\n",
    "        \n",
    "        i = 0\n",
    "        # Loop over all batches\n",
    "        for start, end in zip(range(0, len(train_id), batch), range(batch, len(train_id)+1, batch)):\n",
    "            video_input_data, label_data = video_frames[train_id][start:end], object_label_one_hot[train_id][start:end]\n",
    "\n",
    "            audio_input_data = audio_frames[train_id][start:end]\n",
    "            audio_X = tf.get_default_graph().get_tensor_by_name(\"Model/audio_InputData:0\")\n",
    "\n",
    "            haptic_input_data = haptic_frames[train_id][start:end]\n",
    "            haptic_X = tf.get_default_graph().get_tensor_by_name(\"Model/haptic_InputData:0\")\n",
    "\n",
    "            _, new_cost, cost_scalar = sess.run(\n",
    "                [model_dict[\"Optimizer\"], model_dict[\"Loss\"], model_dict[\"Loss_scalar\"]], \n",
    "                feed_dict={\n",
    "                    video_X: video_input_data, \n",
    "                    audio_X: audio_input_data, \n",
    "                    haptic_X: haptic_input_data, \n",
    "                    Y: label_data, \n",
    "                    audio_keep_prob: 0.5, \n",
    "                    haptic_keep_prob: 0.5\n",
    "                    }\n",
    "                )\n",
    "            # Compute average loss\n",
    "            avg_cost_list += new_cost/total_batch\n",
    "\n",
    "            summary_writer.add_summary(cost_scalar, epoch * total_batch + i)\n",
    "            i += 1\n",
    "        save_path = saver.save(sess, model_path, epoch)\n",
    "         \n",
    "        # Calculate Accuracy\n",
    "        avg_accuracy_list = 0.0\n",
    "        total_batch = int(len(test_id)/batch)\n",
    "        i = 0\n",
    "        for start, end in zip(range(0, len(test_id), batch), range(batch, len(test_id)+1, batch)):\n",
    "            video_input_data, label_data = video_frames[test_id][start:end], object_label_one_hot[test_id][start:end]\n",
    "\n",
    "            audio_input_data = audio_frames[test_id][start:end]\n",
    "            audio_X = tf.get_default_graph().get_tensor_by_name(\"Model/audio_InputData:0\")\n",
    "\n",
    "            haptic_input_data = haptic_frames[test_id][start:end]\n",
    "            haptic_X = tf.get_default_graph().get_tensor_by_name(\"Model/haptic_InputData:0\")\n",
    "\n",
    "            accuracy, accuracy_scalar = sess.run([model_dict[\"Accuracy\"], model_dict[\"Accuracy_scalar\"]], feed_dict={video_X: video_input_data, audio_X: audio_input_data, haptic_X: haptic_input_data, Y: label_data, audio_keep_prob: 1.0, haptic_keep_prob: 1.0})\n",
    "            # Compute average accuracy\n",
    "            avg_accuracy_list += accuracy/total_batch\n",
    "            summary_writer.add_summary(accuracy_scalar, epoch * total_batch + i)\n",
    "            i += 1\n",
    "        \n",
    "        # Printing current epoch accuracy\n",
    "        epoch_cost_accuracy = []\n",
    "        epoch_cost_accuracy.append(time_taken(start_time, time.time()))\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \", Time: \", time_taken(start_time, time.time()))\n",
    "            a_string = \"Cost - \"\n",
    "            epoch_cost_accuracy.append(epoch+1)\n",
    "            \n",
    "            a_string += str(avg_cost_list)\n",
    "            epoch_cost_accuracy.append(str(avg_cost_list))\n",
    "            \n",
    "            a_string = a_string[0:-2]+\" --> Accuracy - \"\n",
    "            a_string += str(avg_accuracy_list)\n",
    "            epoch_cost_accuracy.append(str(avg_accuracy_list))\n",
    "            \n",
    "            print(a_string)\n",
    "        \n",
    "        # Writing current epoch data\n",
    "        with open(logs_path+folder_name+\"_data.csv\", 'a') as f: # append to the file created\n",
    "            writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "            writer.writerow(epoch_cost_accuracy)\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    end_time = time.time()\n",
    "    print(\"Time taken: day, hour, minutes, seconds->\", time_taken(start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taheBWu21g_M",
    "outputId": "f4dc0491-810b-4c06-c71c-8d7f98cf1a3e"
   },
   "outputs": [],
   "source": [
    "print(\"Results are saved in the file \" + logs_path+folder_name+\"_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "MultimodalNetworkTrainingEMIL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
