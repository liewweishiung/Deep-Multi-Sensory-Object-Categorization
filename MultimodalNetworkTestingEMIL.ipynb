{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOoafdQUC_eH"
   },
   "source": [
    "# Multimodal Classification Testing\n",
    "\n",
    "This notebook restores the multimodal network trained for grasp testset1 stored in `./dataset/grasp_testset1_logs` and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko8nR7rBC_eO"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "`Python 3.5.4` is used for development and following packages are required to run the code provided in the notebook:\n",
    "\n",
    "`pip install googledrivedownloader`<br>\n",
    "`pip install matplotlib`<br>\n",
    "`pip install tensorflow-gpu`<br>\n",
    "`pip install keras`<br>\n",
    "`pip install numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ALo1qX2DVC0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywn9HH-YDVxc"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15.0\n",
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr9-921FC_eP"
   },
   "outputs": [],
   "source": [
    "import pickle, os, csv, time, shutil\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "path=\"/content/drive/MyDrive/Deep-Multi-Sensory-Object-Categorization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjChTvZgC_eR"
   },
   "outputs": [],
   "source": [
    "def print_image(image, title):\n",
    "    \"\"\"Print the image\n",
    "\n",
    "    :param image: image pixels in list\n",
    "    :param title: title as string to be printed on top of the image\n",
    "    \"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def time_taken(start, end):\n",
    "    \"\"\"Human readable time between `start` and `end`\n",
    "\n",
    "    :param start: time.time()\n",
    "    :param end: time.time()\n",
    "    :returns: day:hour:minute:second\n",
    "    \"\"\"\n",
    "    time = end-start\n",
    "    day = time // (24 * 3600)\n",
    "    time = time % (24 * 3600)\n",
    "    hour = time // 3600\n",
    "    time %= 3600\n",
    "    minutes = time // 60\n",
    "    time %= 60\n",
    "    seconds = time\n",
    "    day_hour_min_sec = str('%02d' % int(day))+\":\"+str('%02d' % int(hour))+\":\"+str('%02d' % int(minutes))+\":\"+str('%02d' % int(seconds))\n",
    "    \n",
    "    return day_hour_min_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HBU739uC_eT"
   },
   "source": [
    "## Video Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2HnbRXAC_eV"
   },
   "outputs": [],
   "source": [
    "file_0 = path+\"/dataset/EMILver1_preprocessed/EMILver1_vi_vgg16fc2_pca19/arr_0.npy\"\n",
    "file_1 = path+\"/dataset/EMILver1_preprocessed/EMILver1_vi_vgg16fc2_pca19/arr_1.npy\"\n",
    "file_2 = path+\"/dataset/EMILver1_preprocessed/EMILver1_vi_vgg16fc2_pca19/arr_2.npy\"\n",
    "video_frames = np.load(file_0, allow_pickle=True)\n",
    "action_label = np.load(file_1, allow_pickle=True)\n",
    "object_label = np.load(file_2, allow_pickle=True)\n",
    "for i in range(len(video_frames)):\n",
    "    a01 = video_frames[i]\n",
    "    while len(a01) < 658:\n",
    "        a01 = np.concatenate((a01, np.zeros((1, a01.shape[1]))))\n",
    "    video_frames[i] = a01\n",
    "video_frames = np.array(list(video_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtVDkGQyC_eZ"
   },
   "source": [
    "## Sound Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6RnaGs9C_ea"
   },
   "outputs": [],
   "source": [
    "file_0 = path+\"/dataset/EMILver1_preprocessed/EMILver1_au_features/arr_0.npy\"\n",
    "file_1 = path+\"/dataset/EMILver1_preprocessed/EMILver1_au_features/arr_1.npy\"\n",
    "file_2 = path+\"/dataset/EMILver1_preprocessed/EMILver1_au_features/arr_2.npy\"\n",
    "audio_frames = np.load(file_0, allow_pickle=True)\n",
    "action_label = np.load(file_1, allow_pickle=True)\n",
    "object_label = np.load(file_2, allow_pickle=True)\n",
    "for i in range(len(audio_frames)):\n",
    "    a01 = audio_frames[i]\n",
    "    while len(a01) < 658:\n",
    "        a01 = np.concatenate((a01, np.zeros((1, a01.shape[1]))))\n",
    "    audio_frames[i] = a01\n",
    "audio_frames = np.array(list(audio_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkQOlOMLC_ec"
   },
   "source": [
    "## Haptic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yp0UMLXvC_ed"
   },
   "outputs": [],
   "source": [
    "file_0 = path+\"/dataset/EMILver1_preprocessed/EMILver1_sm_features/arr_0.npy\"\n",
    "file_1 = path+\"/dataset/EMILver1_preprocessed/EMILver1_sm_features/arr_1.npy\"\n",
    "file_2 = path+\"/dataset/EMILver1_preprocessed/EMILver1_sm_features/arr_2.npy\"\n",
    "haptic_frames = np.load(file_0, allow_pickle=True)\n",
    "action_label = np.load(file_1, allow_pickle=True)\n",
    "object_label = np.load(file_2, allow_pickle=True)\n",
    "for i in range(len(haptic_frames)):\n",
    "    a01 = haptic_frames[i]\n",
    "    while len(a01) < 658:\n",
    "        a01 = np.concatenate((a01, np.zeros((1, a01.shape[1]))))\n",
    "    haptic_frames[i] = a01\n",
    "haptic_frames = np.array(list(haptic_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxvFLOTqDxE2"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "num_classes = np.nanmax(action_label)+1\n",
    "action_label_one_hot = np.zeros((len(action_label), num_classes)).astype(int)\n",
    "for i in range(len(action_label)):\n",
    "    action_label_one_hot[i, action_label[i]] = 1\n",
    "\n",
    "num_classes = np.nanmax(object_label)+1\n",
    "object_label_one_hot = np.zeros((len(object_label), num_classes)).astype(int)\n",
    "for i in range(len(object_label)):\n",
    "    object_label_one_hot[i, object_label[i]] = 1\n",
    "\n",
    "# train-test-split\n",
    "num_data = len(object_label)\n",
    "train_id, test_id = train_test_split(np.array(range(num_data)), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FuUAnwmC_ee"
   },
   "source": [
    "## Building the Multimodal Network Architecture\n",
    "\n",
    "<img src=\"pics/Multimodal.png\" alt=\"drawing\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHSroT1QC_ee"
   },
   "source": [
    "## Multimodal Network Hyper-parameters\n",
    "\n",
    "This network was trained for 300 epochs using Adam optimization with learning rate 1 x $10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boRGu9WkC_ef",
    "outputId": "e46e0f00-8aa5-4f59-ec7c-61281945d51c"
   },
   "outputs": [],
   "source": [
    "# Network hyper-parameters\n",
    "batch = 1\n",
    "training_epochs = 300\n",
    "display_step = 1\n",
    "\n",
    "behavior = \"grasp\"\n",
    "testset = \"testset1\"\n",
    "folder_name = behavior+'_'+testset\n",
    "model_path = path+\"/dataset/\"+folder_name+\"_logs/model.ckpt\"\n",
    "logs_path = path+\"/dataset/\"+folder_name+\"_logs/\"\n",
    "\n",
    "# num_classes = category_label_train_one_hot.shape[1]\n",
    "num_classes = object_label_one_hot.shape[1]\n",
    "\n",
    "Y = tf.placeholder('float', [None, num_classes], name='LabelData')\n",
    "print(\"Y: \", Y)\n",
    "\n",
    "video_frames_max = 658\n",
    "video_size = video_frames.shape[2]\n",
    "video_X = tf.placeholder('float', [None, video_frames_max, video_size], name='InputData')\n",
    "\n",
    "audio_frames_max = 658\n",
    "audio_size = audio_frames.shape[2]\n",
    "audio_keep_prob = tf.placeholder_with_default(1.0, shape=(), name='audio_keep')\n",
    "\n",
    "haptic_frames_max = 658\n",
    "haptic_size = haptic_frames.shape[2]\n",
    "haptic_keep_prob = tf.placeholder_with_default(1.0, shape=(), name='haptic_keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwxpWllGC_eh",
    "outputId": "982c2ea1-ef3a-4b7d-c624-f8097c7d7ebb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions used to define models\n",
    "\"\"\"\n",
    "\n",
    "haptic_skip_2nd_maxpool = [\"grasp\", \"hold\", \"low\"]\n",
    "\n",
    "def model(video_data_placeholder):\n",
    "    with tf.name_scope(\"Model\"):\n",
    "        # Video\n",
    "        net = tf.layers.flatten(video_data_placeholder)\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net)\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net)\n",
    "        video_logits = tf.layers.dense(inputs=net, units=num_classes, activation=tf.nn.relu)\n",
    "        \n",
    "        # Audio\n",
    "        audio_data_placeholder = tf.placeholder('float', [None, audio_frames_max, audio_size], name='audio_InputData')\n",
    "        net = tf.layers.flatten(audio_data_placeholder)\n",
    "        # Dense Layer\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net, rate=audio_keep_prob)\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net, rate=audio_keep_prob)\n",
    "        audio_logits = tf.layers.dense(inputs=net, units=num_classes, activation=tf.nn.relu)\n",
    "        \n",
    "        # Haptic\n",
    "        haptic_data_placeholder = tf.placeholder('float', [None, haptic_frames_max, haptic_size], name='haptic_InputData')\n",
    "        net = tf.layers.flatten(haptic_data_placeholder)\n",
    "        # Dense Layer\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net, rate=haptic_keep_prob)\n",
    "        haptic_logits = tf.layers.dense(inputs=net, units=num_classes, activation=tf.nn.relu)\n",
    "        \n",
    "        # Concatenate \n",
    "        logits = tf.concat([video_logits, audio_logits, haptic_logits], axis=1)\n",
    "        logits = tf.nn.relu(logits)\n",
    "        logits = tf.layers.dense(inputs=logits, units=num_classes)\n",
    "        \n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(prediction, label_placeholder):\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=label_placeholder))\n",
    "        # Create a summary to monitor cost tensor\n",
    "        cost_scalar = tf.summary.scalar(\"loss\", cost)\n",
    "    return cost, cost_scalar\n",
    "\n",
    "def training(prediction, label_placeholder):\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = optimizer.minimize(cost)\n",
    "    return train_op\n",
    "\n",
    "def evaluate(prediction, Y):\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        accuracy_scalar = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    return accuracy, accuracy_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLSp1FFbC_ei",
    "outputId": "331dd6ac-00e5-412d-bcf7-b67221a4c159"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating the Neural Network\n",
    "\"\"\"\n",
    "\n",
    "model_dict = {}\n",
    "prediction = model(video_X)\n",
    "model_dict[\"Model\"] = prediction\n",
    "\n",
    "cost, cost_scalar = loss(prediction, Y)\n",
    "model_dict[\"Loss\"] = cost\n",
    "model_dict[\"Loss_scalar\"] = cost_scalar\n",
    "\n",
    "train_op = training(prediction, Y)\n",
    "model_dict[\"Optimizer\"] = train_op\n",
    "\n",
    "eval_op, accuracy_scalar = evaluate(prediction, Y)\n",
    "model_dict[\"Accuracy\"] = eval_op\n",
    "model_dict[\"Accuracy_scalar\"] = accuracy_scalar\n",
    "\n",
    "print(\"model_dict: \", model_dict)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iILTcC8C_ei"
   },
   "source": [
    "## Testing for the whole clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXdMUuq5C_ei",
    "outputId": "51d4dcb1-e9a2-489a-baa0-5c154a727e8f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(logs_path))\n",
    "    \n",
    "    avg_accuracy_list = 0.0\n",
    "    total_batch = int(len(test_id)/batch)\n",
    "    \n",
    "    for start, end in zip(range(0, len(test_id), batch), range(batch, len(test_id)+1, batch)):\n",
    "        video_input_data, label_data = video_frames[start:end], object_label_one_hot[start:end]\n",
    "\n",
    "        audio_input_data = audio_frames[start:end]\n",
    "        audio_X = tf.get_default_graph().get_tensor_by_name(\"Model/audio_InputData:0\")\n",
    "\n",
    "        haptic_input_data = haptic_frames[start:end]\n",
    "        haptic_X = tf.get_default_graph().get_tensor_by_name(\"Model/haptic_InputData:0\")\n",
    "    \n",
    "        accuracy = sess.run(model_dict[\"Accuracy\"], \n",
    "                            feed_dict={\n",
    "                                video_X: video_input_data, \n",
    "                                audio_X: audio_input_data, \n",
    "                                haptic_X: haptic_input_data, \n",
    "                                Y: label_data, \n",
    "                                audio_keep_prob: 1.0, \n",
    "                                haptic_keep_prob: 1.0\n",
    "                            }\n",
    "                           )\n",
    "        \n",
    "        avg_accuracy_list += accuracy/total_batch\n",
    "            \n",
    "    print(\"Overall Accuracy: \", avg_accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N0ZkTxGC_ej"
   },
   "source": [
    "## Testing for every frame in video and for every 5th frame in audio and haptic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5j-_oj6mC_ej",
    "outputId": "0761dd71-140c-41f0-845d-9fac3613cfab"
   },
   "outputs": [],
   "source": [
    "# Predict after every frame in video and for every 5th frame in audio and haptic\n",
    "\n",
    "all_frames_acc = {}\n",
    "for a_frame in range(1, video_frames_max+1):\n",
    "    all_frames_acc[a_frame] = [0, 0, 0] # correct, wrong, percentage\n",
    "\n",
    "y_pred = {}\n",
    "y_score = {}\n",
    "for a_frame in range(1, video_frames_max+1):\n",
    "    y_pred[a_frame] = []\n",
    "    y_score[a_frame] = []\n",
    "    \n",
    "def fill_example(example, fill_frames, a_frame):\n",
    "    temp = []\n",
    "    temp.append(example[a_frame-1])\n",
    "    for _ in range(fill_frames):\n",
    "        example = np.concatenate((example, temp))\n",
    "    return example\n",
    "\n",
    "def give_me_complete_clip(num_of_frame, example, frame_list):\n",
    "    fill_frames = frame_list - num_of_frame\n",
    "    if fill_frames != 0:\n",
    "        example = fill_example(example, fill_frames, num_of_frame)\n",
    "    return example\n",
    "\n",
    "category_labels = {\"soft-blue-ball\": 1, \"soft-red-ball\": 2, \"soft-orange-ball\": 3, \"hard-pink-ball\": 4, \"light-red-car\": 5, \"blue-car\": 6, \"yellow-car\": 7, \"heavy-green-car\": 8, \"light-red-sponge\": 9, \"light-blue-sponge\": 10, \"blue-tissues\": 11, \"pink-tissues\": 12, \"soft-banana\": 13, \"light-hard-banana\": 14, \"heavy-hard-banana\": 15, \"hard-small-banana\": 16, \"green-frog\": 17, \"purple-duck\": 18, \"orange-fish\": 19, \"yellow-seal\": 20, \"small-yellow-die\": 21, \"big-yellow-die\": 22, \"light-soft-apple\": 23, \"light-hard-apple\": 24, \"heavy-soft-apple\": 25, \"heavy-hard-apple\": 26, \"heavy-blue-toy\": 27, \"red-toy\": 28, \"yellow-toy\": 29, \"heavy-green-toy\": 30}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(logs_path))\n",
    "    \n",
    "    for i_example in range(len(test_id)):\n",
    "        print(\"Example Number: \", i_example+1)\n",
    "        video_input_data, label_data = video_frames[test_id][i_example], object_label_one_hot[test_id][i_example]\n",
    "        \n",
    "        audio_input_data = audio_frames[test_id][i_example]\n",
    "        \n",
    "        haptic_input_data = haptic_frames[test_id][i_example]\n",
    "        \n",
    "        truth = np.argmax(label_data, 0)+1\n",
    "        truth_cate = list(category_labels.keys())[list(category_labels.values()).index(truth)]\n",
    "        print(\"True Category: \", truth_cate)\n",
    "        \n",
    "        for a_frame in range(1, video_frames_max+1):\n",
    "            try:\n",
    "                video_example = give_me_complete_clip(a_frame, video_input_data[0:a_frame], video_frames_max)\n",
    "                video_temp = []\n",
    "                video_temp.append(video_example)\n",
    "\n",
    "                frame5Hz = a_frame*5 # For every frame in video there are 5 frames in audio and haptic\n",
    "\n",
    "                audio_example = give_me_complete_clip(frame5Hz, audio_input_data[0:frame5Hz], audio_frames_max)\n",
    "                audio_temp = []\n",
    "                audio_temp.append(audio_example)\n",
    "\n",
    "                haptic_example = give_me_complete_clip(frame5Hz, haptic_input_data[0:frame5Hz], haptic_frames_max)\n",
    "                haptic_temp = []\n",
    "                haptic_temp.append(haptic_example)\n",
    "\n",
    "                audio_X = tf.get_default_graph().get_tensor_by_name(\"Model/audio_InputData:0\")\n",
    "                haptic_X = tf.get_default_graph().get_tensor_by_name(\"Model/haptic_InputData:0\")\n",
    "\n",
    "                predic = sess.run(model_dict[\"Model\"], feed_dict={video_X: video_temp, audio_X: audio_temp, haptic_X: haptic_temp, audio_keep_prob: 1.0, haptic_keep_prob: 1.0})\n",
    "                argmax = sess.run(tf.argmax(predic, 1))+1\n",
    "                cate = list(category_labels.keys())[list(category_labels.values()).index(argmax)]\n",
    "                print(\"Video Frame No.: \", a_frame, \"Audio/Haptic Frame No.: \", frame5Hz, \"Prediction: \", cate)\n",
    "\n",
    "                y_pred[a_frame].extend(argmax)\n",
    "                y_score[a_frame].extend(predic)\n",
    "\n",
    "                if truth_cate == cate:\n",
    "                    all_frames_acc[a_frame][0] += 1\n",
    "                else:\n",
    "                    all_frames_acc[a_frame][1] += 1\n",
    "                all_frames_acc[a_frame][2] = all_frames_acc[a_frame][0]/(all_frames_acc[a_frame][0]+all_frames_acc[a_frame][1])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        print(\"X\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0JwLgfuC_ej"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Writing a CSV file that records accuracy for each frame\n",
    "\"\"\"\n",
    "\n",
    "with open(logs_path+folder_name+\"_acc_VEveryFrame_AH5Frame.csv\",'w') as f:\n",
    "    writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "    \n",
    "    first_row = [\"Frame_No\", \"Correct\", \"Wrong\", \"Accuracy\"]\n",
    "    writer.writerow(first_row)\n",
    "    for a_frame in range(1, video_frames_max+1):\n",
    "        row = [a_frame]\n",
    "        row.append(all_frames_acc[a_frame][0])\n",
    "        row.append(all_frames_acc[a_frame][1])\n",
    "        row.append(all_frames_acc[a_frame][2])\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "MultimodalNetworkTestingEMIL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
